\documentclass [11pt, twocolumn] {article}
%\usepackage[small, compact]{titlesec}
\usepackage[vmargin=0.25in,hmargin=0.6in]{geometry}
\usepackage{amsmath}

\begin{document} 

\title { TaskSprint: A general purpose distributed computation system }
\author{ Sergio Benitez \and  Ivan Sergeev \and Alec Thomson  }
\date {}

\maketitle

\section {Introduction} 
TaskSprint is a general-purpose, fault-tolerant, distributed system for deterministic and non-deterministic computation. TaskSprint differentiates itself from existing systems, like MapReduce and Pig, by allowing non-deterministic computations, allowing arbitrary stage computations (as opposed to two in MapReduce), having no central point of failure (as opposed to Pig/MapReduce's master), being language agnostic, and being computation agnostic.

TaskSprint's core challenge is to provide all of these facilities in a fault-tolerant manner while exposing minimal complexity to the user. To this end, TaskSprint's only requirement from the user are two small pieces of software: a scheduler and a node. The scheduler's job is to define a schedule for tasks, and the node's job is implement those tasks. All the intermediate work is handled by the system. This includes distributing and replicating the computation across nodes in the network, handling data shuffling efficiently, handling task dependencies, and dealing with scheduler, node, and network failures.

\section {Design}

TaskSprint's infrastructure is centered on two major components: the coordinator and the client. The coordinator communicates with the user's scheduler, and the client communicate's with the user's nodes. The coordinator and the client communicate with each other to execute the user's computation. To support replicated, non-deterministic computations, we seed PRNGs with the same value.

\subsection {Coordinator} 

The coordinator is a Paxos-replicated, event-driven state-machine that responds to clients' requests for the current task schedule. The replication ensures that the system continues as long as a majority of the replicas are capable of communicating. 

Clients query the coordinator for the latest ``view'', the task schedule, and notify the coordinator when tasks are completed by nodes. A view includes a mapping of tasks to clients in addition to the parameters for each task as defined by the user's scheduler. Parameters include task name, task arguments, and task dependencies. 

The coordinator monitors the liveliness of a clients. When a client hasn't queried the coordinator for a certain period of time, the coordinator marks the client as dead and reassigns its tasks. A single replica known as the ``leader'' periodically inserts ``TICK'' operations into the Paxos log to ensure that all replicas exactly agree on when clients died in relation to other operations. New leaders are periodically elected via Paxos to handle leader failure.

The coordinator communicates events to the user's scheduler. In response to events, the scheduler can start new tasks, kill existing tasks, or end the computation. These event handlers are very simple and easy to implement while providing developers a large amount of flexibility for their computations. For instance, a genetic algorithm scheduler is able to group similar solutions for crossover and a Bitcoin mining scheduler is able to kill tasks to free resources when solutions are found.

\subsection {Client} 

A client periodically queries the coordinator for new views and checks for changes to the task assignments. When it finds an assignment to itself that it hasn't seen before, it calls developer-provided task functions to actually perform the task. If the task specifies specific pre-requisite tasks, the client library handles moving copying data from other clients and provides this data as input to the developer-provided function. Our client library also handles setting the provided random seeds so that developer provided code can operate as if it were completely non-deterministic. 

When finished with a task, a client will contact the coordinator and maintain the task's output data until the task is killed. Finally, when the coordinator returns a view marked ``finished'', the client can discard its data and gracefully exit. 

\section {Examples}

\subsection {Multiple Root Finder} 

\newcommand{\abs}[1]{\lvert{#1}\rvert}

A root-finding algorithm finds $x$ for which a function $f(x)$ is 0. While many root-finding algorithms exist, few are equipped to search for multiple roots of a function without new instantiations of the algorithm initialized with different initial guesses. Our distributed multiple root finder searches a wide solution space stochastically, obviating the need for initial guesses, and evolves multiple clusters of the ``fittest'' roots to likely roots within a specified epsilon. The search is defined by a simple set of parameters, including a function of an arbitrary number of variables, a bit resolution, solution space dimensions, solution epsilon, root closeness epsilon, and a search time. It is implemented with a genetic algorithm that runs as a client task, whose initial population is strategically selected by the multiple root finder coordinator.

The genetic algorithm task evolves a local population of candidate roots subject to fitness function $-\abs{f(x)}$, a crossover function that produces a child $x$ from parents $x_1$ and $x_2$ by means of an arithmetic average, and a mutation function that randomly flips the lower bits in the bitwise representation of $x$. The multiple root finder coordinator merges the fittest populations that are clustered near a possible root $x$ into a new genetic algorithm task for further refinement, announces a found root $x$ when its fitness is within the solution epsilon, and launches new genetic algorithm tasks until its search time expires. The coordinator discards the results of new searches in the neighborhood of previously found roots, only evolving populations near unseen roots.

\subsection {Bitcoin Miner}

Bitcoin mining is the act of cryptographically chaining a new block of Bitcoin transactions to the canonical block chain of existing transactions, by manipulating the new block until its SHA256 hash contains a certain number of leading zeros. It is called mining because the miner is rewarded in newly minted Bitcoins for his or her valid solution. A miner forms a new block from uncommitted transactions advertised on the Bitcoin network, a transaction to him or herself for the current mining reward, and some other fields, including a nonce initialized to zero. The miner increments this nonce until the hash of the whole block contains the prerequisite number of leading zeros. If the miner finds this solution before another miner does, he or she advertises the solved block to the network, which will eventually accept it in the canonical block chain, thereby rewarding the miner.

Our Bitcoin miner client task performs the repeated nonce iteration and SHA256 hashing of a potential block, reporting the solution nonce to the coordinator if it is found. Our Bitcoin miner coordinator starts tasks with new block data and a starting nonce value. The miner coordinator partitions the 32-bit nonce value space evenly among the number of available clients, starting each with a different nonce value. If a solution is reported by a task, the miner coordinator submits the block to the network, stops all current tasks attempting to solve that block, and starts new tasks with a new block and starting nonces. If a solution to the block is found by a competing miner, our minor coordinator stops all current tasks, and starts new tasks with a new block and starting nonces.

\subsection {MapReduce} 

MapReduce is a model of computation that processes a list of key-value pairs into a result in a highly parallelizable fashion. The model is based on two phases of computation: $\text{map}(k, v) \rightarrow k',v'$ which accepts a input key-value pair and yields an intermediate key-value pair, and $\text{reduce}(k', v') \rightarrow r$ which accepts an intermediate key-value pair and yields a result. The map and reduce phases are defined arbitrarily by the particular application, but must be pure functions, which permits their parallel scheduling among many workers. Map tasks are run concurrently across all input key-values, and then reduce tasks are run concurrently on all intermediate key-values. Reduce tasks results are collected by a coordinator to yield the final result of the computation.

Our implementation of MapReduce includes a map client task, reduce client task, and a MapReduce coordinator. The MapReduce coordinator is responsible spawning map tasks with an input key-value pair (or just an input key, if the corresponding value is large and stored on a shared file system), and for spawning the subsequent reduce tasks with their input as the result of a pending or completed map task. Intermediate results between map and reduce phases are moved by the clients alone, avoiding a data bottleneck at the coordinator. Final results from the reduce tasks are collected at the MapReduce coordinator at the end of the computation. Our MapReduce implementation handles failed tasks gracefully, with the built-in task replication of TaskSprint and a rescheduling policy, to ensure the computation completes despite individual task failures.

\end{document}
