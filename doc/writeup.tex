\documentclass [11pt, twocolumn] {article}
%\usepackage[small, compact]{titlesec}
\usepackage[vmargin=0.5in,hmargin=0.5in]{geometry}
\usepackage{amsmath}

\begin{document} 

\title { TaskSprint: A general purpose distributed computation system }
\author{ Alec Thomson \and Sergio Benitez \and Ivan Sergeev }
\date {}

\maketitle


\section {Introduction} 

MapReduce is a popular distributed computation system because it provides a very simple programming interface for performing large computations over clusters of machines. Unfortunately, MapReduce has some limitations that prevent it from being a good solution for certain problems. These limitations include a vulnerable master server, no support for non-deterministic computations, and a slow synchronization period between the map and reduce phases. To solve these limitations, we developed TaskSprint, a fault-tolerant distributed computation system with a high degree of flexibility.  

Solving the limitations of MapReduce is a difficult problem because increasing the generality of MapReduce runs the risk of exposing developers to a more complex and unwieldy programming interface. To solve this problem, we hide as much of the operation of the system from developers as we can without breaking the generality of our system. Developers define tasks by writing simple functions in a programming language of their choice. Developers also define their own coordination functions with a simple language-agnostic interface. While requiring developers to provide their own coordinators decreases the usability of the system, we believe most developers will rely on pre-written coordinators that solve common tasks. The flexibility provided by our system allows it to be used for a large variety of distributed computation tasks. 

\section {Design}

TaskSprint is split into two major components, a replicated coordinator and a client program. Each of these components communicates with developer-provided functions to perform general purpose tasks. Seeds for random number generators are deterministically passed between machines in the background to support non-deterministic computations such as genetic algorithms. 

\subsection {Coordinator} 

The coordinator is a Paxos-replicated event-driven server that handles assignment of tasks to individual clients. The replication ensures that the system continues as long as a majority of the replicas are capable of communicating. Clients query the coordinator for the latest ``View'' of task assignments and notify the coordinator when tasks are completed. This view includes a mapping of task IDs to clients and the parameters for each task ID (which include task name, a random seed, and a list of pre-requisite tasks that provide additional input). 

When a client hasn't queried the coordinator during a certain period of time, the coordinator marks the client as dead and reassigns its tasks. A single replica known as the ``leader'' periodically inserts ``TICK'' operations into the Paxos log to ensure that the replicas agree exactly when clients died in relation to other operations. New leaders are periodically elected via Paxos to handle leader failure. 

Our coordinator communicates events to a developer-provided event handler. This handler receives notifications of finished tasks along with short pieces of output data (requested when the task started) and can then tell the coordinator to start new tasks, kill existing tasks, or end the computation. These event handlers are very simple and easy to implement while providing developers a large amount of flexibility for their computations. For instance, the genetic algorithm handler is able to group similar solutions for crossover and the Bitcoin mining handler is able to kill tasks to free resources when solutions are found. 

\subsection {Client} 

A client periodically queries the coordinator for new views and checks for changes to the task assignments. When it finds an assignment to itself that it hasn't seen before, it calls developer-provided task functions to actually perform the task. If the task specifies specific pre-requisite tasks, the client library handles moving copying data from other clients and provides this data as input to the developer-provided function. Our client library also handles setting the provided random seeds so that developer provided code can operate as if it were completely non-deterministic. 

When finished with a task, a client will contact the coordinator and maintain the task's output data until the task is killed. Finally, when the coordinator returns a view marked ``finished'', the client can discard its data and gracefully exit. 

\section {Examples}

\subsection {Multiple Root Finder} 

\newcommand{\abs}[1]{\lvert{#1}\rvert}

A root-finding algorithm finds $x$ for which a function $f(x)$ is 0. While many root-finding algorithms exist, few are equipped to search for multiple roots of a function without new instantiations of the algorithm initialized with different initial guesses. Our distributed multiple root finder searches a wide solution space stochastically, obviating the need for initial guesses, and evolves multiple clusters of the ``fittest'' roots to likely roots within a specified epsilon. The search is defined by a simple set of parameters, including: a function of an arbitrary number of variables, a bit resolution, solution space dimensions, solution epsilon, root closeness epsilon, and a search time. It is implemented with a genetic algorithm that runs as a client task, whose initial population is strategically selected by the multiple root finder coordinator.

The genetic algorithm task evolves a local population of candidate roots subject to fitness function $-\abs{f(x)}$, a crossover function that produces a child $x$ from parents $x_1$ and $x_2$ by means of an arithmetic average, and a mutation function that randomly flips bits in the bitwise representation of $x$. The multiple root finder coordinator merges the fittest populations that are clustered near a possible root $x$ into a new genetic algorithm task for further refinement, announces a found root $x$ when its fitness is within the solution epsilon, and launches new genetic algorithm tasks until its search time expires. The coordinator discards the results of new searches in the neighborhood of previously found roots, only evolving populations near unseen roots.

\subsection {Bitcoin Miner}

Bitcoin mining is the act of cryptographically chaining a new block of Bitcoin transactions to the canonical block chain of existing transactions, by manipulating the new block until its SHA256 hash contains a certain number of leading zeros. It is called mining because the miner is rewarded in newly minted Bitcoins for his or her valid solution. A miner forms a new block from uncommitted transactions advertised on the Bitcoin network, a transaction to him or herself for the current mining reward, and some other fields, including a nonce initialized to zero. The miner increments this nonce until the hash of the whole block contains the prerequisite number of leading zeros. If the miner finds this solution before another miner does, he or she advertises the solved block to the network, which will eventually accept it in the canonical block chain, thereby rewarding the miner.

Our Bitcoin miner client task performs the repeated nonce iteration and SHA256 hashing of a potential block, reporting the solution nonce to the coordinator if it is found. Our Bitcoin miner coordinator starts tasks with new block data and a starting nonce value. The miner coordinator partitions the 32-bit nonce value space evenly among the number of available clients, starting each with a different nonce value. If a solution is reported by a task, the miner coordinator submits the solved block to the network, stops all current tasks attempting to solve that block, and starts new tasks with a new block and starting nonces. If a block expires, our minor coordinator stops all current tasks attempting to solve that block, and starts new tasks with a new block and starting nonces.

\subsection {MapReduce} 

MapReduce is a model of computation that processes a list of key-value pairs into a result in a highly parallelizable fashion. The model is based on two phases of computation: $\text{map}(k, v) \rightarrow k',v'$ which accepts a input key-value pair and yields an intermediate key-value pair, and $\text{reduce}(k', v') \rightarrow r$ which accepts an intermediate key-value pair and yields a result. The map and reduce phases are defined arbitrarily by the particular application, but must be pure functions, which permits their parallel scheduling among many workers. Map tasks are run concurrently across all input key-values, and then reduce tasks are run concurrently on all intermediate key-values. Reduce tasks results are collected by a coordinator to yield the final result of the computation.

Our implementation of MapReduce includes a map client task, reduce client task, and a MapReduce coordinator. The MapReduce coordinator is responsible spawning map tasks with an input key-value pair (or just an input key, if the corresponding value is large and stored on a shared file system), and for spawning the subsequent reduce tasks with their input as the result of a pending or completed map task. Intermediate results between map and reduce phases are moved by the clients alone, avoiding a data bottleneck at the coordinator. Final results from the reduce tasks are collected at the MapReduce coordinator at the end of the computation. Our MapReduce implementation handles failed tasks gracefully, with the built-in task replication of TaskSprint and a rescheduling policy, to ensure the computation completes despite individual task failures.

\end{document}
